---
title: "Assignment2"
author: "Irene Fernández Rebollo i Àlex Martorell i Locascio"
date: "5/12/2021"
output:
  pdf_document: null
  toc: yes
  word_document: default
html_document:
  toc: yes
toc_float:
  collapsed: no
smooth_scroll: no
df_print: paged
editor_options:
  chunk_output_type: console
---
  
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, echo=FALSE}
rm(list=ls())
setwd("C:/Users/Alex/Desktop/UPC/1st Semester/Statistical Inference and Modelling/SIM_Assignment2")
options(contrasts=c("contr.treatment","contr.treatment"))
#setwd("~/Desktop/SIM/Assignment2")
```

# Presentation
A company which is active in Big Data and Data Science wants to hire data scientists among people who successfully pass some courses which are conducted by the company. Many people signup for their training. Company wants to know which of these candidates really want to work for the company after training or looking for a new employment because it helps to reduce the cost and time as well as the quality of training or planning the courses and categorization of candidates. Information related to demographics, education, experience are in hands from candidates signup and enrollment.

This dataset designed to understand the factors that lead a person to leave current job for HR researches too. By model(s) that uses the current credentials, demographics, experience data you will predict the probability of a candidate to look for a new job or will work for the company, as well as interpreting affected factors on employee decision.


# Data Preparation

```{r}
df <- read.csv("aug_train.csv",header=T, sep=",", na.strings="NA")
summary(df)
```

```{r message=FALSE, warning=FALSE, echo=FALSE}
# Introduce required packages:

requiredPackages <- c("car","lmtest", "FactoMineR","car", "factoextra","RColorBrewer","ggplot2","dplyr","knitr", "corrplot", "mvoutlier", "chemometrics", "MASS", "effects", "tidyverse", "ROCR", "rms", "fmsb")


#use this function to check if each package is on the local machine
#if a package is installed, it will be loaded
#if any are not, the missing package(s) will be installed and loaded
package.check <- lapply(requiredPackages, FUN = function(x) {
  if (!require(x, character.only = TRUE)) {
    install.packages(x, dependencies = TRUE)
    library(x, character.only = TRUE)
  }
})
#verify they are loaded
# search()
```


## Removing Duplicates and Irrelavant Observations
```{r}
df <- unique(df) #No duplicates
#nrow(df) == nrow(unique(df)) 

set.seed(130798) #Birthday of 1 member of the group as random seed:
samples <- as.vector(sort(sample(1:nrow(df),5000))) #Subset of 5000 observations
df <- df[samples,]
```


## Fix structural errors
For coding purposes, blanks ("") in the dataframe are considered as NA's.
```{r, results='hide'}
sum(is.na(df)) #0
df[df==""] <- NA
sum(is.na(df)) #5433
```
Some inconsistencies are also checked and corrected.
```{r}
# Column name relevent_experience should be relevant_experience
colnames(df)[5] <- "relevant_experience"
# Also their values should be with the "relevant" word
df$relevant_experience <- gsub("relevent", "relevant", df$relevant_experience)

# Correct the format of the company size
df$company_size[df$company_size == "10/49"] <- "10-49"
```


## Univariate Descriptive Analysis
Out of the 14 variables in the dataset, R detects 9 of them being character-type.
They are transformed into factors, taking into consideration NA values.
One of the more tricky aspects is the years of experience variable. Two possibilites are taken into consideration,
a factor with levels based on quartiles and a numeric variable.

### City code
```{r, results='hide'}
class(df$city) #"character"
df$city <- factor(df$city) #Transform to "factor"
```
```{r, fig.height=14, fig.width=10, echo=FALSE}
ggplot(df,aes(x = forcats::fct_infreq(city), fill = city)) + 
  geom_bar(stat = 'count', width = 0.8) +
  scale_y_continuous(expand = c(0, 0)) + 
  scale_fill_discrete(guide = "none") +
  coord_flip() + xlab("City") + ylab("Count") + 
  theme_minimal()
```

### Developement index of the city
```{r, results='hide'}
class(df$city_development_index) #"numeric"
# Kept as "numeric"
```
```{r, fig.height=2, echo=FALSE}
ggplot(df, aes(x = city_development_index)) +
  geom_boxplot(notch = TRUE) + 
  theme_minimal()
```

### Gender of candidate
```{r, results='hide'}
class(df$gender) #"character"
df$gender <- factor(df$gender) #Transform to "factor"
```
```{r, fig.height=3, echo=FALSE}
ggplot(df,aes(x = gender, fill = gender)) + 
	geom_bar(stat = 'count', width = 0.8) + 
  scale_fill_manual(values = c("hotpink", "lightskyblue", "gold"), guide = "none") +
  xlab("Gender") + ylab("Count") +
  theme_minimal()
```

### Relevant experience of candidate
```{r, results='hide'}
class(df$relevant_experience) #"character"
df$relevant_experience <- factor(df$relevant_experience, 
                                 levels=c("Has relevant experience", "No relevant experience"),
                                 labels = c("Yes", "No")) #Transform to "factor" and change to simpler values
```
```{r, fig.height=3, echo=FALSE}
ggplot(df,aes(x = relevant_experience, fill = relevant_experience)) + 
	geom_bar(aes(y = (..count..)/sum(..count..)*100), width = 0.8) + 
  scale_fill_manual(values = c("green3", "red2"), guide = "none") +
  xlab("Relevant experience") + ylab("Percentage (%)") + ylim(c(0, 100)) +
  theme_minimal()
```

### Type of University course enrolled
```{r, results='hide'}
class(df$enrolled_university) #"character"
df$enrolled_university <- factor(df$enrolled_university, 
                                levels=c("no_enrollment", "Part time course", "Full time course"), 
                                labels= c("No", "Part-Time", "Full-Time")) #Transform to "factor" and change to simpler values
```
```{r, fig.height=4, echo=FALSE}
ggplot(df,aes(x = enrolled_university, fill = enrolled_university)) + 
	geom_bar(stat = 'count', width = 0.8) + 
  scale_fill_manual(values  = c("red3", "steelblue", "darkgreen"), guide = "none") +
  xlab("Type of course") + ylab("Count") + theme_minimal()
```

### Education level of candidate
```{r, results='hide'}
class(df$education_level) #"character"
df$education_level <- factor(df$education_level,
                            levels = c("Primary School", "High School",
                                      "Graduate", "Masters", "Phd")) #Transform to "factor"
```
```{r, fig.height=3, echo=FALSE}
ggplot(df,aes(x = education_level, fill = education_level)) + 
	geom_bar(stat = 'count', width = 0.8) + 
  scale_fill_discrete(guide = "none") +
  xlab("Education level") + ylab("Count") + 
  theme_minimal()
```

### Education major discipline of candidate
```{r, results='hide'}
class(df$major_discipline) #"character"
df$major_discipline <-factor(df$major_discipline,
                             levels=c("STEM", "Business Degree", "Arts", 
                                      "Humanities", "No Major", "Other")) #Transform to "factor"
```
```{r, fig.height=3, echo=FALSE}
ggplot(df,aes(x = major_discipline, fill = major_discipline)) + 
	geom_bar(stat = 'count', width = 0.8) + 
  scale_fill_discrete(guide = "none") +
  xlab("Education major discipline") + ylab("Count") +
  theme_minimal()
```

### Candidate total experience in years
```{r, results='hide'}
class(df$experience) #"character"
df$experience <- factor(df$experience) #Transform to "factor"
```
```{r, fig.height=3.5, echo=FALSE}
ggplot(df,aes(x = experience, fill = experience)) + 
	geom_bar(stat = 'count', width = 0.8) + 
  xlab("Experience") + ylab("Count") +
  scale_fill_discrete(guide = "none") +
  theme_minimal()
```

### Number of employees in current employer's company
```{r, results='hide'}
class(df$company_size) #"character"
df$company_size <- factor(df$company_size) #Transform to "factor"
```
```{r, fig.height=3, echo=FALSE}
ggplot(df,aes(x = company_size, fill = company_size)) + 
  geom_bar(stat = 'count', width = 0.8) + 
  xlab("Company size") + ylab("Count") +
  scale_fill_discrete(guide = "none") +
  theme_minimal()
```

### Type of current employer
```{r, results='hide'}
class(df$company_type) #"character"
df$company_type <- factor(df$company_type) #Transform to "factor"
```
```{r, fig.height=3, echo=FALSE}
ggplot(df,aes(x = company_type, fill = company_type)) + 
  geom_bar(stat = 'count', width = 0.8) + 
  xlab("Company type") + ylab("Count") +
  scale_fill_discrete(guide = "none") +
  theme_minimal()
```

### Difference in years between previous job and current job
```{r, results='hide'}
class(df$last_new_job) #"character"
df$last_new_job <-factor(df$last_new_job) #Transform to "factor"
```
```{r, fig.height=3, echo=FALSE}
ggplot(df,aes(x = last_new_job, fill = last_new_job)) + 
  geom_bar(stat = 'count', width = 0.8) + 
  xlab("Years passed from previous job") + ylab("Count") +
  scale_fill_discrete(guide = "none") +
  theme_minimal()
```

### Training hours completed
```{r, results='hide'}
class(df$training_hours) #"integer"
#Keep as "integer"
```
```{r, fig.height=2, echo=FALSE}
ggplot(df,aes(x = training_hours)) + 
  geom_boxplot(notch = TRUE) + 
  xlab("Training hours")+
  theme_minimal()
```

## Missing values
The percentage of missing values per column with respect to their total varies significantly along different variables. \texttt{company\_type} and \texttt{company\_size} have over 30% of missing values. The numeric variables, the target, \texttt{relevant\_experience}, and \texttt{city} return no missing values.

```{r}
missing <- unlist(lapply(df, function(x) sum(is.na(x))))/nrow(df)
sort(missing[missing >= 0], decreasing = TRUE)
count_na<-colSums(is.na(df[, 1:14]))
count_na
```
```{r, results='hide'}
sum(is.na(df)) #5433
```


## Outliers

### Univariate Outlier detection
Outliers for the three numerical variables are considered. Training hours and city_development_index are fairly straightforward to analyze, but with regards to the experience variable there are no outliers due to the manner in which the variable was built. The characters "<1" and ">20" do not allow enough precision in order to identify for possible outliers, as they had to be converted to numbers 0 and 21 respectively during the conversion to a numeric variable (noting a significant bias).

The outliers found are registered in two different ways. A separate table is built for the Data Quality Report named \texttt{dqind} (stands for Data Quality per Individual). This table stores the rows where  mild and extreme outliers for training hours and mild outliers for city development index occur. A new variable called \texttt{quality} is added to the main data frame also for the Data Quality Report.

```{r, fig.height=4}
par(mfrow = c(1,2))
Boxplot(df$city_development_index)
Boxplot(df$training_hours)
```
```{r, results='hide'}
mout_city = quantile(df$city_development_index)[[2]]-1.5*IQR(df$city_development_index)
sum(df$city_development_index < mout_city) #6
mout_th = quantile(df$training_hours)[[4]]+1.5*IQR(df$training_hours)
eout_th = quantile(df$training_hours)[[4]]+3*IQR(df$training_hours)
sum(df$training_hours > mout_th) #227
sum(df$training_hours > eout_th) #58

```

### Multivariate outlier detection
With the two numerical variables, we apply the Moutlier function at 99.5%. 105 multivariate outliers are returned.
```{r}
res.out<-Moutlier(df[,c(3,13)],quantile=0.995)
```
```{r, results='hide'}
#quantile(res.out$md,seq(0,1,0.005))
multout<-which(res.out$md > res.out$cutoff)
length(multout) #105
```
```{r, echo=FALSE}
par(mfrow = c(1, 1))
plot( res.out$md, res.out$rd )
text(res.out$md, res.out$rd, labels=rownames(df),adj=1, cex=0.5)
abline(h=res.out$cutoff, col="red")
abline(v=res.out$cutoff, col="red")
```

## Data Quality Report
To summarize all the its and buts of the Data Processing in a readable manner, two dataframes are created, one refers to variables and the other to individuals. This is the \textit{Data Quality Report}. For obvious reasons, it is possible to print out the whole data quality report per variable but not per individual (5000 in total). Also, a data quality variable is created.
```{r}
df$quality <- 0
```

### Per variable
```{r}
dqvar <- data.frame(colnames(df[, 1:14]))
dqvar$outliers <-0
dqvar$missing <-0
dqvar$errors <-0

# Outliers
dqvar[13, "outliers"] <- sum(df$training_hours > mout_th) + sum(df$training_hours > eout_th)
dqvar[3, "outliers"] <- sum(df$city_development_index < mout_city)

#Missing
#We add missing values to the column per variable 
dqvar$missing <- (colSums(is.na(df[, 1:14])))
dqvar
```

### Per individual
```{r}
dqind <- data.frame(df$enrollee_id)
colnames(dqind)[1] <- "enrollee_id"
dqind$missing <-0
dqind$outliers <-0
dqind$errors <-0

#Outliers
regmout_th <- subset(df$enrollee_id, df$training_hours > mout_th)
regeout_th <-subset(df$enrollee_id, df$training_hours > eout_th)
regmout_city <-subset(df$enrollee_id, df$city_development_index < mout_city)

for(i in 1:length(regmout_th)) {
  s<-which(df$enrollee_id == regmout_th[i])
  w<-which(dqind$enrollee_id == regmout_th[i])
  df[s, "quality"] <- df[s , "quality"] + 1
  dqind[w, "outliers"] <- dqind[w, "outliers"] + 1}

for(i in 1:length(regeout_th)) {   # for-loop over rows
  s<-which(df$enrollee_id == regeout_th[i])
  w<-which(dqind$enrollee_id == regeout_th[i])
  df[s, "quality"] <- df[s , "quality"] + 1
  dqind[w, "outliers"] <- dqind[w, "outliers"] + 1}

for(i in 1:length(regmout_city)) {   # for-loop over rows
  s<-which(df$enrollee_id == regmout_city[i])
  w<-which(dqind$enrollee_id == regmout_city[i])
  df[s , "quality"] <- df[s , "quality"] + 1
  dqind[w, "outliers"] <- dqind[w, "outliers"] + 1}

df[multout, "quality"] <- df[multout, "quality"] + 1
dqind[multout, "outliers"] <- dqind[multout, "outliers"] + 1

dqind$missing <- rowSums(is.na(df))
df$quality <- df$quality + rowSums(is.na(df))
head(dqind)
```

Note that the quality variable is the sum of missing values and outliers. Using the \texttt{condes()} package we obtain the correlation with the quantitative
and qualitative variables. Note that company_size and company_type as factors seem to influence the most on the quality variable (R2 around 0.6). That is 
because of the large missing value number that these variables have.
```{r}
# Correlation
table(df$quality)
cor(df[, c(3, 13:15)])

res.con <- condes(df, 15)
res.con$quali
res.con$quanti
```


## Imputation of values
There are 14 variables and 5000 individuals; for each individual we can have from 0 to 7 NA's along the variables.
We can obtain an histogram were we can observe that there is a tail for higher number of missing values, so we determine a first rule of thumb that is that any row containing 5 missing values or more will be removed because of the cost that results when imputing so many values and only represents aprox. 1% of the samples.
```{r}
#Create column with na counts for every observation
df$na_count <- rowSums(is.na(df))

# Actualitzem aquí el Data Quality per Missing's
for(i in 1:nrow(df)) {   # for-loop over rows
  df[i , "quality"] <- df[i , "quality"] + df[i, "na_count"]
  #w <- which(dqind$enrollee_id == df[i, "enrollee_id"])
  dqind[i, "missing"] <- df$na_count[i]}
```
```{r, echo=FALSE, fig.height=3}
ggplot(df, aes(x = na_count, y = (..count..)/sum(..count..)*100)) +
  geom_histogram(binwidth = 1, color = "black", fill = "lightblue") +
  xlab("Number of NA's") + ylab("Percentage (%)") +
  theme_minimal()
```
```{r, results='hide'}
df <- df[df$na_count < 5,]
nrow(df) #4946
100 - nrow(df)/5000*100 #only removed a 1.08% of the observations
```

The variables with some NA value are: gender (1184), enrolled_university (93), education_level (115), major_discipline (735), experience (16), company_size (1576), company_type (1608), last_new_job (106). Each one is analysed separately to decide the required imputation method.

* Gender: In this case, there are a lot of NA's. We decide to create a new category "Missing" because probably there are individuals that do not want to share this information.
* Enrolled university: In this case, we cannot assign a new category for NA's since the variable stores already the three possible categories, so we decide to do imputation applying MCA.
* Education level: For this variable we also decide to impute NA's with MCA because we cannot find any similarity of these values with the values of one category.
* Major discipline: Here the "STEM" category is more represented that the rest, so we try to collapse the rest in "Others" since the probabilities in the target are similar between them compared with the "STEM" value; following this approach, the NA's were assigned into "Others"
* Experience: In experience, since we only have 10 missing values, we will use MCA imputation.
* Company size: There are a lot of NA's in this variable, we create another category "Unknown" because is possible that the individual does not know this value.
* Company type: In this case, the number of missing values is very high. We tried to find some pattern in the probabilities of the categories in the target but the NA's are very different from the rest. We decide to assign these NA's values into the "Other" category because the number of individuals in the category are very low and does not make sense to create a "Missing" or "Unknown" category for this variable.
* Last new job: For this variable, analyzing the probabilities we saw some similarity with the category "4", but as we are not sure of this assignation and the number of missing values not exceeds 100, we applied MCA imputation
```{r, results='hide'}
summary(df)
df$target <- factor(df$target, levels=c(1, 0), labels=c("Target.Yes", "Target.No"))
table(df$target)

#Gender
levels(df$gender)[length(levels(df$gender)) + 1] <- "Missing"
df[is.na(df$gender), "gender"] <- "Missing"

#Enrolled university
#levels(df$enrolled_university)[length(levels(df$enrolled_university)) + 1] <- "Missing"
#df[is.na(df$enrolled_university), "enrolled_university"] <- "Missing"
#prop.table(table(df$enrolled_university, df$target)) #missMDA
levels(df$enrolled_university)
nrow(df[is.na(df$enrolled_university),]) #66

#Education level
#levels(df$education_level)[length(levels(df$education_level)) + 1] <- "Missing"
#df[is.na(df$education_level), "education_level"] <- "Missing"
#prop.table(table(df$education_level, df$target)) #missMDA
levels(df$education_level)
nrow(df[is.na(df$education_level),]) #75

#Major discipline
levels(df$major_discipline)[length(levels(df$major_discipline)) + 1] <- "Missing"
df[is.na(df$major_discipline), "major_discipline"] <- "Missing"
prop.table(table(df$major_discipline, df$target)) #STEM & other only?
df[df$major_discipline %in% c("Business Degree", "Arts", "Humanities", "No Major"), "major_discipline"] <- "Other"
table(df$major_discipline)
prop.table(table(df$major_discipline, df$target)) #STEM & other only?
df[df$major_discipline == "Missing", "major_discipline"] <- "Other"
table(df$major_discipline)
prop.table(table(df$major_discipline, df$target)) #STEM & other only?
df$major_discipline <- factor(df$major_discipline)
levels(df$major_discipline)

#Experience
#levels(df$experience)[length(levels(df$experience)) + 1] <- "Missing"
#df[is.na(df$experience), "experience"] <- "Missing"
#prop.table(table(df$experience, df$target)) #missMDA
levels(df$experience)
nrow(df[is.na(df$experience),]) #10

#Company size
levels(df$company_size)[length(levels(df$company_size)) + 1] <- "Unknown"
df[is.na(df$company_size), "company_size"] <- "Unknown"
prop.table(table(df$company_size, df$target)) #unknown

#Company type
#levels(df$company_type)[length(levels(df$company_type)) + 1] <- "Missing"
#df[is.na(df$company_type), "company_type"] <- "Missing"
#prop.table(table(df$company_type, df$target)) #assign to other
df[is.na(df$company_type), "company_type"] <- "Other"

#Last new job
#levels(df$last_new_job)[length(levels(df$last_new_job)) + 1] <- "Missing"
#df[is.na(df$last_new_job), "last_new_job"] <- "Missing"
#prop.table(table(df$last_new_job, df$target)) #missMDA
levels(df$last_new_job)
nrow(df[is.na(df$last_new_job),]) #91
```
For some variables with NA's we use missDNA library to impute them. As it is a process that take some time, the code appear commented to avoid its execution; the resulting dataframe without NA's is stored and we load it in the next step.
```{r, eval=FALSE}
#library(missMDA)
#dfimp <- df
#colnames(dfimp)
#res <- MCA(dfimp[, c(4:12)])
#res <- MCA(dfimp[, c(6, 7, 9, 12)])
#vars_dis <- names(dfimp)[c(6, 7, 9, 12)]
#summary(dfimp[,vars_dis])
#nb <- estim_ncpMCA(dfimp[,vars_dis],ncp.max=25)
#res.input<-imputeMCA(dfimp[,vars_dis],ncp=10)

#Result of Imputation
#summary(res.input$completeObs)
#summary(df)
#df$enrolled_university <- res.input$completeObs$enrolled_university
#df$education_level <- res.input$completeObs$education_level
#df$experience <- res.input$completeObs$experience
#df$last_new_job <- res.input$completeObs$last_new_job
#summary(df)
#write.csv(df, "df_imputation.csv", row.names = FALSE)
```


## Data Profiling

```{r Profiling}
# Check if either training hours or city development index are normally distributed
shapiro.test(df$city_development_index)
shapiro.test(df$training_hours)

# B ~ X where B is the response factor variable and X is the explanatory cont. variable
str(df)
# Test on means
oneway.test(df$city_development_index ~ df$target)
kruskal.test(df$city_development_index ~ df$target)
oneway.test(df$training_hours ~ df$target)
kruskal.test(df$training_hours ~ df$target)

#Test on variances
fligner.test(df$city_development_index ~ df$target)
fligner.test(df$training_hours ~ df$target)

#Catdes
str(df)
res.cat <- catdes(df, 14)
res.cat$test.chi2
res.cat$category
res.cat$quanti.var
res.cat$quanti
```
The following conclusions are reached after the Data Profiling:
*It is observed using the Shapiro-Wilk Test that neither \texttt{city\_development\_index} nor \texttt{training\_hours} follow a normal distribution. 
*It is tested if the Target Factor is impacted by the numerical variables. In other words, does a certain value in these continuous variables affect the outcome.? Observe that even though both available tests are done, the valid results are the ones from Kruskal-Wallis test, since it does not assume normality on the explanatory variable. The output shows a 0 p-value for \texttt{city\_development\_index} and around 0.29 for \texttt{training\_hours}. In the first case it is clear, there is influence on the target depending on the city where the employee resides. For the number of training hours the null hypothesis is not rejected, so the mean is spread equally around the tearget variable
*It is also important to check if dispersion of the numerical variables affects the target, and we get the similar answer as in the means.
*Using the \texttt{catdes()} package from \texttt{FactoMinR} global association between the target factor and categorical variables is assessed. The chi-squared test gives p-values a lot lower than 0.05 for all the factors.
*The \texttt{quanti.var} output shows if the quantitative variables influence the target. Since all p-values are very low, the is answer Yes. Similarly, the \texttt{quanti} output shows if the mean in category varies with the overall mean.

## Interpretation of all the results before modelling

## Separation between Train and Test
```{r}
# We upload the new dataframe with imputation results
df <- read.csv("df_imputation.csv",header=T, sep=",", na.strings="NA")
for(i in 1:ncol(df)){
  if(is.character(df[, i])){
    df[, i] <-factor(df[, i])
  }
}
#summary(df)
df$quality <-NULL
df$na_count <-NULL
```
```{r}
set.seed(130798)
s <- sample(1:nrow(df),round(0.75*nrow(df),0))
dfall<-df
df <- df[s,]
dftest <-dfall[-s,]
```

# Modelling the Target
\textbf{Observation.} To simplify and differentiate kinds of models, the following table states the notation
\begin{center}
\begin{tabular}{ |l|l| } 
\hline
m0 & Null model \\ 
\hline
m1, m11, m12...  & Variations with numerical explanatory variables \\ 
\hline
m2, m21, m22...  & Variations with factor variables  \\ 
\hline
m3, m31, m32... & Variations with numerical and factor variables \\
\hline
\end{tabular}
\end{center}

The first model to try is the null model. A quick verifaction that the intercept in fact corresponds with the logit link function is performed.
```{r}
# First model
m0 <- glm(target ~ 1, family="binomial", data=df)
ptt <- prop.table(table(df$target))
summary(m0)
oddm0 <- ptt[2] / (1- ptt[2])
log(oddm0)
```

## Modelling the target using numeric variables
The numerical variables that we originally have are training hours and city development index.
A significant improvement with regards to the AIC and the Deviance is observed with respect to the 
null model. The step function suggests to remove the training hours variable, as the AIC slightly decreases.
We have labeled the models in this family as m1. Note that a quick overview
of the marginal plots of \texttt{m11} suggests a variable transformation on city
development index. Both a polynomial of degree 2 (\texttt{m12}) and a logarithmic transformation
are assessed (\texttt{m13}), getting a significantly better fit with the latter option, as the marginal plots show. However, the AIC worsens slightly in comparison to the polynomial model, although it is still better than the first model. We choose to keep model \texttt{m13}.
Let us check also for influential observations: The Influence plot shows several
observations with a large Cook's distance (based on the area of the circle).
To filter the important observations, we use a BoxPlot for the Cook's distance
which will yield the 10 instances with the largest Cook's D.It is prudent to delete these observations,
as it is a general way of proceeding with a posteriori influential data. The model is reestimated, and although the AIC decreases, this is not relevant because the data frame has 2 fewer observations. The Residual Deviance also shows a significant drop.

With regards to the final numerical model, a rule of thumb is applied to check
for goodness of fit since the data is not aggregated. Since the residual 
deviance and the degrees of freedom are of the same order, the model can be labeled a good fit.

Finally, with function \texttt{allEffects()} we see the logarithmic effect
of the explanatory variable.
```{r, results='hide'}
m1 <- glm(target ~ . , family="binomial", data=df[, c(3, 13, 14)])
m11<- step(m1) #city_development_index + training_hours
m11 <- step(m1, k=log(nrow(df))) #BIC case   city_development_index
```
```{r, fig.height=4}
marginalModelPlot(m11) #some transformation needed

m12 <- glm(target ~ poly(city_development_index,2), family="binomial", data=df)
m13 <- glm(target ~ log(city_development_index), family="binomial", data=df)
#AIC(m11, m12, m13)

par(mfrow = c(1, 2))
marginalModelPlot(m12)
marginalModelPlot(m13) #better fit

influencePlot(m13)
cook <- Boxplot(cooks.distance(m13))
cookd <- sort(cooks.distance(m13)[cook], decreasing=TRUE)
cookd #10 influent observations
df <- df[!(rownames(df) %in% names(cookd)),]

m131 <- glm(target ~ log(city_development_index), family="binomial", data=df)
AIC(m13, m131)
#summary(m13)
#summary(m131)
marginalModelPlots(m131)

residualPlots(m131) #some values of the tail in m13 are removed
#plot(allEffects((m131)))

# Temporarily remove outliers: compare models
#dfcheck <-df
#ll <- which(df$enrollee_id %in% regmout_th)
#dfcheck <- dfcheck[-ll,]
#ll <- which(df$enrollee_id %in% regeout_th)
#df <- df[-ll, ]
#ll <- which(df$enrollee_id %in% regmout_city)
#dfcheck <- dfcheck[-ll, ]

#m14 <- glm(target ~ log(city_development_index) + training_hours, family="binomial", data=dfcheck)
#AIC(m14, m13)
```

## Introducing factor variables
The first important thing to note here is the difference between the Akaike Information Criterion (AIC from now on) and the Bayesian Information Criterion(BIC). The AIC has many different formulas that describe it, but the main idea is that takes into account the number of parameters in the model and 
the total of number observations. It applies a penalty based on the number of parameters. The BIC, however, applies a larger penalty. So in search of the best model, different results will be achieved according to the criteria used.

First, a general model m2 containing all . A significant drop in comparison with respect to the deviance from the null model is noted, however, there is an egregious amount of parameters and that is not good news. It will be shown that reducing the amount of factors (especially the ones with many factor levels)

In this specific m2 model with the results printed in Appendix C, many p-values 
close to 1 are observed for some factor levels. This is clearly noticeable with 
the city factor. The step functions are now applied, for AIC and BIC criteria respectively. Both eliminate several variables:
* Using AIC criteria we obtain a model that eliminates factors \texttt{experience},  \texttt{company\_type} and \texttt{gender} 
* Using BIC criteria we obtain a model that only considers 3 regressors: 
\texttt{enrolled\_university} , \texttt{major\_discipline} and \texttt{company\_size}. We choose the latter model because it will be much
easier to check for factor interactions in the enxt section.
The \texttt{Anova()} function gives low p-values for all regresssors, which
suggests a good fit.
gives got results with the latter model, showing that all regressors are useful.
Let us follow with a bit of interpretation:

```{r, results='hide'}
m00 <-glm(target ~ 1, family="binomial", data=df)
summary(m00)
m2 <- glm(target ~ ., family="binomial", data=df[, c(2, 4:12, 14)])
AIC(m2);BIC(m2)

# AIC option
#m21<-step(m2, trace=0)
#m21$anova
#summary(m21)

#BIC option
m211<-step(m2, k=log(nrow(df)), trace=0)
m211$anova
```
```{r}
summary(m211)
#vif(m211)
#Anova(m21, test="LR")
#Anova(m211, test="LR")

#AIC(m00, m211)
plot(allEffects(m211), axes=list(y=list(lab="target")))
```


### Factor interactions
We go ahead and study some more properties for this model. Note that all the factors are additive, and we want to check interaction. We have four factors,
and we calculate the different factor interactions: $$ {3 \choose 2}=3 $$
Firstly, two by two factor interactions show that gender and en
```{r, results='hide'}
check <- as.numeric(df$target) - 1

# enrolled_university - major_discipline
#with(df, interaction.plot(enrolled_university, major_discipline, check, lwd = 2, col = 1:2))
m22 <- glm(target ~ enrolled_university + major_discipline, family="binomial", data=df)
m23 <- glm(target ~ enrolled_university * major_discipline, family="binomial", data=df)
Anova(m23, test="LR") #No interaction

# enrolled_university - company_size
with(df, interaction.plot(enrolled_university, company_size, check, lwd = 2, col = 1:9))
m22 <- glm(target ~ enrolled_university + company_size, family="binomial", data=df)
m23 <- glm(target ~ enrolled_university * company_size, family="binomial", data=df)
Anova(m23, test="LR") #No interaction, not clear...

prop.table(table(df$enrolled_university, df$company_size), 2)
table(df$company_size)
df$company_size_col <- fct_collapse(df$company_size, "<1000"= c("<10", "10-49", "50-99", "100-500", "500-999"), ">1000" = c("1000-4999", "5000-9999", "10000+"))

table(df$company_size_col)
with(df, interaction.plot(enrolled_university, company_size_col, check, lwd = 2, col = 1:9))
# We try enrolled_university *company_size_col 
m24 <- glm(target ~ enrolled_university * company_size_col, family="binomial", data=df)
Anova(m24, test="LR")
AIC(m23, m24)
summary(m23)
summary(m24)
BIC(m23, m24)
BIC(m2)

# major_discipline - company_size
#with(df, interaction.plot(major_discipline, company_size, check, lwd = 2, col = 1:2))
m22 <- glm(target ~ major_discipline + company_size, family="binomial", data=df)
m23 <- glm(target ~ major_discipline * company_size, family="binomial", data=df)
Anova(m23, test="LR") #No interaction
```
The following table summarizes the different possibilities factor interactions:
\begin{center}
\begin{tabular}{ |l||r|r|r| } 
\hline
Models & Deviance & n-p & AIC \\ 
\hline
\texttt{enrolled\_university + major\_discipline} & 3946.5 & 3696 & 3954.5 \\
\hline
\texttt{enrolled\_university * major\_discipline} & 3942.8 & 3694 & 3954.8 \\
\hline
\texttt{enrolled\_university + company\_size} & 3834 & 3689 & 3856\\
\hline
\texttt{enrolled\_university * company\_size} & 3809 & 3673 & 3863 \\
\hline
\texttt{major\_discipline + company\_size} & 3813.5 & 3690 & 3833.5 \\
\hline
\texttt{major\_discipline * company\_size} & 3804.3 & 3682 & 3840.3 \\
\hline
\end{tabular}
\end{center}



## Best model
We end up with the best three models so far. The table below summarizes these models according to their Residual Deviance and their BIC. We end up chooosing
model \texttt{m13} because it has the lowest BIC.
```{r}
m3 <- glm(target ~ log(city_development_index) + enrolled_university + major_discipline + company_size, family="binomial", data=df)
summary(m3)

m31 <- glm(target ~ log(city_development_index) + enrolled_university + major_discipline + company_size_col, family="binomial", data=df)
summary(m31)


m32 <- glm(target ~ log(city_development_index) + enrolled_university * company_size_col + major_discipline, family="binomial", data=df)
summary(m32)

anova(m31, m32, test="Chisq")

BIC(m3, m31, m32)


marginalModelPlot(m31)
residualPlots(m31)
influencePlot(m31)
cook <- Boxplot(cooks.distance(m31))
cookd <- sort(cooks.distance(m31)[cook], decreasing=TRUE)
cookd

df<-df[!(rownames(df) %in% names(cookd)),]

m4 <- glm(target ~ log(city_development_index) + enrolled_university + major_discipline + company_size_col, family="binomial", data=df)

```
\begin{center}
\begin{tabular}{ |l||r|r| } 
\hline
Model & Deviance & BIC  \\ 
\hline
\texttt{m3} & 3462.6 & 3569.37\\
\hline 
\texttt{m31} & 3470.1 &  3527.602\\
\hline
\texttt{m32} & 3459.5 & 3459.870 \\
\hline
\end{tabular}
\end{center}

## Analysis, Goodness of Fit for our Final model.
First, we output all the important analysis and results for a final model.
```{r}
marginalModelPlot(m4)
residualPlots(m4)
influencePlot(m4)
avPlots(m4)
summary(m4)
df[c("4040"),]
df[c("4715"),]
sum( resid( m4, "pearson") ^2 )

marginalModelPlots(m4,id=list(labels=row.names(df),method=abs(cooks.distance(m4)), n=5) )

avPlots(m4,id=list(labels=row.names(df),method=abs(cooks.distance(m4)), n=5) )

crPlots(m4,id=list(labels=row.names(df),method=abs(cooks.distance(m4)), n=5) )

residualPlots(m4, layout=c(3, 2))
outlierTest(m4)
par(mfrow=c(1,1))
influencePlot(m4,id=list(n=5) )

model.final <- lrm(target ~ log(city_development_index) + enrolled_university + major_discipline + company_size_col,  data=df)
model.final

m0 <- glm(target ~ 1, family="binomial", data=df)
NagelkerkeR2(m4)
summary(m4)
100*(1-m4$dev/m4$null.dev)
m4$dev
m4$null.dev


dadesroc<-prediction(predict(m4,type="response"),df$target)
par(mfrow=c(1,2))
plot(performance(dadesroc,"err"))
plot(performance(dadesroc,"tpr","fpr"), colorize=TRUE)
abline(0,1,lty=2)

library(cvAUC)
AUC(predict(m4,type="response"),df$target)

```
1.	The summary for the final model shows good p-values for most intercepts, except for the company size bigger than 1000. 
2.	With regards to the ROC curve, we seem to get a decent fit. The AUC (Area under curve) is 0.77, which is a labeled as a good fit by statisticians. 
3.	The Nagel-Kerke test returns a pseudo-coefficient of determination of 0.24. Not knowing if this a good fit, we compare it with Lab 6 Election 92 Diagnostics, that presented a final model that returned an R2 of 0.25, so our fit seems adequate. 
4.	In the previous section when we considered the best three models so far, we deleted a posteriori influential data based on the Cook’s distance value. In the end, our test data frame has ended up with 3690 observations (20 less than in the beginning)
5.	In the Marginal Model Plots we see that up until the linear predictor is 0, the model seems quite a good fit. The problem is that the Model expects that with higher values in the linear predictor, the target should be 1. However, our data has values with high linear predictor that have are Target 0. This introduced a bias in our model and that is why the data curve deviates from the model curve.
6.	The Residual Plots for the log(city development index) variable show very high Pearson Residuals as the city development index gets bigger. This because our model is not predicting well the Target variable if the city development index increases.
7.	If we look at the Influential Plot, we see many a priori influential values, but find less a posteriori influential values. That is because they may have high leverage, but the Cook’s Distance is not high enough for them to be removed. 
8.	The Added Variable Plots consistently show observations 4040 and 4715 with very high Pearson residuals in most of the plots. 
We observe that the city development index is very high (0.92 and 0.93) respectively. But there is no other similarity between these instances that allows us to draw any conclusion as to why they systematically appear as outliers.
9.	The component residual plots do not add any more important information. Again, it is shown that problems appear with high logarithm city development index values, as the residuals are far away from the pink line which is the optimal fit.
10.	The final conclusion is that although these aforementioned values negatively impact our model, since we cannot label them as influential a posteriori data, we cannot directly remove them. This is the best model obtained considering all the improvements.


## Forecasting capability of the final model
```{r}
df.fin <- m4
dftest$company_size_col <- fct_collapse(dftest$company_size, "<1000"= c("<10", "10-49", "50-99", "100-500", "500-999"), ">1000" = c("1000-4999", "5000-9999", "10000+") )
job.vot <- predict(df.fin, newdata=dftest, type="response")
job.est <- ifelse(job.vot < 0.5, 0, 1)
table(job.est, dftest$target)
sum(diag(table(job.est,dftest$target)))/dim(dftest)[1]

# Null model
m0<-glm(target ~ 1, family=binomial, data=dftest)
job.vot0 <- m0$fit
job.est0 <- ifelse(job.vot0<0.5,0,1)
table(job.est0,dftest$target)
table(job.est0,dftest$target)[1,2]/dim(dftest)[1]

library(ResourceSelection)
hoslem.test(dftest$target, job.vot)



```
# Appendix
This last section includes extra information or plots that are not part of 
the main report in an attempt to avoid overloading the plot 
## Appendix A: More on Data Preparation

## Appendix B: More on the Target Modelling using Covariates

## Appendix C: More on the Target Moddeling using Factors
```{r}
summary(m2)
```
